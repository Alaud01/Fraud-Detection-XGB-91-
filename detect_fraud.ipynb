{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Import required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import stats\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, roc_curve, auc, confusion_matrix, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Define Classes and Functions\n",
    "# Create class for preprocessing, which will include steps like reformatting, ensuring correct data-types, etc.\n",
    "class preprocessDatasets:\n",
    "  def join_ID(self, transaction_df: pd.DataFrame, ID_df: pd.DataFrame):  \n",
    "    ''' \n",
    "    Function to join datasets based on transaction ID.\n",
    "\n",
    "    Parameters:\n",
    "    -transaction_df (pd.Dataframe): transaction dataset\n",
    "    -ID_df (pd.Dataframe): identity dataset\n",
    "\n",
    "    Returns:\n",
    "    -merged_df (pd.Dataframe): merged transaction & identity dataset\n",
    "    '''\n",
    "\n",
    "    merged_df = pd.merge(transaction_df, ID_df, on='TransactionID', how='outer')\n",
    "    col_list = []\n",
    "    for col in merged_df.columns:\n",
    "      if '-' in col:  # Replace '-' and '_' since column names between the two datasets does not align\n",
    "        col = col.replace('-', '_')\n",
    "      col_list.append(col)\n",
    "    merged_df.columns = col_list\n",
    "    return merged_df\n",
    "\n",
    "  def replace_blanks(self, df: pd.DataFrame):\n",
    "    ''' \n",
    "    Function to replace blanks with \"holder\" value of -999.\n",
    "\n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): dataset\n",
    "\n",
    "    Returns:\n",
    "    -df (pd.Dataframe): dataset with no blank values\n",
    "    '''\n",
    "\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):  # Check if the column is numeric\n",
    "            df[col].fillna(-999, inplace=True)\n",
    "\n",
    "        else:\n",
    "            df[col].fillna('-999', inplace=True)\n",
    "\n",
    "    return df\n",
    "  \n",
    "  def encode_df(self, df: pd.DataFrame):\n",
    "    ''' \n",
    "    Encode/scale dataframe columns to maintain data quality prior to feeding into model.\n",
    "\n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): dataset to be encoded/scaled\n",
    "\n",
    "    Returns:\n",
    "    -df (pd.Dataframe): encoded & scaled dataset\n",
    "    '''\n",
    "\n",
    "    # Create instances of the transformers\n",
    "    label_encoder = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "\n",
    "    # Loop through each column and determine its type, then encode/scale accordingly\n",
    "    for col in df.columns:\n",
    "      if col == 'TransactionID' or col == 'isFraud':  # These two columns can be left as is, and will be needed for processing later\n",
    "          continue\n",
    "      \n",
    "      # MinMax Scale D-columns as indicated from the plots and correlation matrix\n",
    "      elif re.match(r'^D\\d+$', col):\n",
    "          df[col] = minmax_scaler.fit_transform(df[[col]])\n",
    "\n",
    "      # Check if the column is numeric\n",
    "      elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "          df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "      # Process non-numeric columns (object types)\n",
    "      else:\n",
    "          df[col] = label_encoder.fit_transform(df[col].astype(str))  # Convert to string if necessary\n",
    "\n",
    "    return df\n",
    "  \n",
    "  def remove_outliers(self, data: pd.DataFrame, column: str):\n",
    "    '''\n",
    "    Remove outliers in transaction amount col that are >3sigma based on a single column, in this case transaction amount.\n",
    "\n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): dataset to be encoded/scaled\n",
    "    -column (str): reference column to remove outliers\n",
    "\n",
    "    Returns:\n",
    "    -data (pd.Dataframe): dataset with rows contained outliers removed\n",
    "    '''\n",
    "    \n",
    "    z_scores = stats.zscore(data[column])  # z-score each of the columns\n",
    "    non_outliers = np.abs(z_scores) < 3\n",
    "    original_len = len(data)  # calculate to initial length to note count of values removed\n",
    "    data = data[non_outliers]\n",
    "    final_len = len(data)  # calculate to final length to note count of values removed\n",
    "    print(f'{original_len - final_len} values were removed, since they contianed outliers (z-score over 3)')\n",
    "    return data\n",
    "  \n",
    "  def remove_empty_cols(self, data: pd.DataFrame):\n",
    "    '''\n",
    "    Drop col if more than 90% of the col is missing (NaN).\n",
    "\n",
    "    Parameters:\n",
    "    -data (pd.Dataframe): dataset to be processed\n",
    "\n",
    "    Returns:\n",
    "    -data (pd.Dataframe): dataset where columns exceeding the blank-threshold are dropped\n",
    "    '''\n",
    "\n",
    "    threshold = 0.90\n",
    "    data.dropna(thresh=int((1 - threshold) * len(data)), axis=1, inplace=True)\n",
    "    return data\n",
    "  \n",
    "  def feature_engineer(self, data: pd.DataFrame):\n",
    "    '''\n",
    "    Feature engineer dataset to extract more details from certain columns.\n",
    "\n",
    "    Parameters:\n",
    "    -data (pd.Dataframe): dataset to be processed\n",
    "\n",
    "    Returns:\n",
    "    -df (pd.Dataframe): dataset where columns exceeding the blank-threshold are dropped\n",
    "    '''\n",
    "\n",
    "    # OS col contains multiple versions, which are minor and should be treated the same\n",
    "    # id_33 includes screen size, split for width and height\n",
    "    # id_31 has browser info\n",
    "    # DeviceInfo contains type of device, etc.\n",
    "    data[['P_emailserver', 'P_suffix']] = data['P_emaildomain'].str.split('.', n=1, expand=True)\n",
    "    data[['R_emailserver', 'R_suffix']] = data['R_emaildomain'].str.split('.', n=1, expand=True)\n",
    "    data['os'] = data['id_30'].str.split(' ', expand=True)[0]\n",
    "    data['screen_width'] = data['id_33'].str.split('x', expand=True)[0]\n",
    "    data['screen_height'] = data['id_33'].str.split('x', expand=True)[1]\n",
    "    data['screen_width'] = pd.to_numeric(data['screen_width'])\n",
    "    data['screen_height'] = pd.to_numeric(data['screen_height'])\n",
    "\n",
    "    # Modify browser identification\n",
    "    data['browser'] = data['id_31'].str.split(' ', expand=True)[0].str.lower()\n",
    "    data['device_name'] = data['DeviceInfo'].str.split(' ', expand=True)[0].str.lower()\n",
    "\n",
    "    def matchPatterns(df: pd.DataFrame, patterns, col_name: str):\n",
    "      for pattern, value in patterns.items():\n",
    "        # Apply regex pattern to match and replace values in the column\n",
    "        df[col_name] = df[col_name].str.replace(pattern, value, regex=True)\n",
    "      return df\n",
    "\n",
    "    browser_patterns = {\n",
    "      r'samsung/sm-g532m|samsung/sch|samsung/sm-g531h': 'samsung',\n",
    "      r'generic/android': 'android',\n",
    "      r'mozilla/firefox': 'firefox',\n",
    "      r'nokia/lumia': 'nokia',\n",
    "      r'zte/blade': 'zte',\n",
    "      r'lg/k-200': 'lg',\n",
    "      r'lanix/ilium': 'lanix',\n",
    "      r'blu/dash': 'blu',\n",
    "      r'm4tel/m4': 'm4'\n",
    "    }\n",
    "\n",
    "    device_patterns = {\n",
    "      r'samsung|sgh|sm|gt-': 'samsung',\n",
    "      r'mot': 'motorola',\n",
    "      r'ale-|.*-l|hi': 'huawei',\n",
    "      r'lg': 'lg',\n",
    "      r'rv:': 'rv',\n",
    "      r'blade': 'zte',\n",
    "      r'xt': 'sony',\n",
    "      r'iphone': 'ios',\n",
    "      r'lenovo': 'lenovo',\n",
    "      r'mi|redmi': 'xiaomi',\n",
    "      r'ilium': 'ilium',\n",
    "      r'alcatel': 'alcatel',\n",
    "      r'asus': 'asus'\n",
    "    }\n",
    "\n",
    "    data = matchPatterns(data, browser_patterns, 'browser')\n",
    "    data = matchPatterns(data, device_patterns, 'device_name')\n",
    "\n",
    "    # Modify date format to split day, month, etc.\n",
    "    start_date = datetime(2017, 11, 30)\n",
    "    data['TransactionFullDate'] = data['TransactionDT'].apply(lambda x: start_date + timedelta(seconds=x))\n",
    "    data['TransactionDate'] = data['TransactionFullDate'].dt.date\n",
    "    data['DayOfWeek'] = data['TransactionFullDate'].dt.dayofweek.apply(lambda x: (x + 1) % 7)    # Sunday=0, Monday=1, Tuesday=2, etc\n",
    "    data['HourOfDay'] = data['TransactionFullDate'].dt.hour   # from 0 to 23\n",
    "    data['Month'] = data['TransactionFullDate'].dt.month\n",
    "    return data\n",
    "  \n",
    "  def reduce_memory(self, df: pd.DataFrame):\n",
    "    '''\n",
    "    Reduce memory by analyzing max/min value in cols and minimizing column data type\n",
    "\n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): dataset used for memory reduction\n",
    "\n",
    "    Returns:\n",
    "    -df (pd.Dataframe): dataset with reduced memory\n",
    "    '''\n",
    "\n",
    "    start = df.memory_usage().sum() / 1024**2  # Get initial memory\n",
    "    print('Starting memory usage of the dataframe: {:.2f} MB'.format(start))\n",
    "    \n",
    "    for col in df.columns:  # Apply memory reduction steps\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if str(col_type).startswith('float'):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "\n",
    "        elif str(col_type).startswith('int'):\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    end = df.memory_usage().sum() / 1024**2  # Get final memory\n",
    "    print('Memory usage after downsizing: {:.2f} MB'.format(end))\n",
    "    print('Memory usage decreased by {:.1f}%'.format(100 * (start - end) / start))\n",
    "    return df\n",
    "\n",
    "  def final_preprocessing(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    '''\n",
    "    Apply final preprocessing steps such as dropping cols, sorting, reformatting, etc.\n",
    "    \n",
    "    Parameters:\n",
    "    -train_df (pd.Dataframe): training dataset to be used by the model\n",
    "    -test_df (pd.Dataframe): testing dataset to be used by the model\n",
    "\n",
    "    Returns:\n",
    "    -train_df (pd.Dataframe): fully processed training dataset\n",
    "    -dropped_df (pd.Dataframe): fully processed testing dataset\n",
    "    '''\n",
    "\n",
    "    # Sort values to keep predictions in submission format\n",
    "    train_df = train_df.sort_values(by='TransactionID', ascending=True).reset_index(drop=True)\n",
    "    test_df = test_df.sort_values(by='TransactionID', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    train_df.drop(columns=['P_emaildomain', 'R_emaildomain', 'id_30', 'id_31', 'id_33', 'DeviceInfo', 'TransactionDT', 'TransactionFullDate', 'TransactionDate', 'TransactionID'], inplace=True)\n",
    "    test_df.drop(columns=['P_emaildomain', 'R_emaildomain', 'id_30', 'id_31', 'id_33', 'DeviceInfo', 'TransactionDT', 'TransactionFullDate', 'TransactionDate', 'TransactionID'], inplace=True)\n",
    "\n",
    "    # Due to removal of cols with >90% missing values, the column removals need to be carried over to test dataset as well\n",
    "    col_drop =[]\n",
    "    for col in test_df.columns:\n",
    "       if col not in train_df.columns:\n",
    "          col_drop.append(col)\n",
    "    \n",
    "    dropped_df = test_df.drop(columns=col_drop)\n",
    "    return train_df, dropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class visualizeDataset:\n",
    "  def plot_hist(self, df: pd.DataFrame, col: str, plot_fraud: bool):\n",
    "    '''\n",
    "    Plot histogram of transactions for fraud/non-fraud etc.\n",
    "    \n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): training dataset to be visualized\n",
    "    -col (str): column to use for plotting histogram\n",
    "    -plot_fraud (bool): boolean to choose whether fraud and non-fraud transaction should be plotted separately\n",
    "\n",
    "    Displays: Plots as needed\n",
    "    '''\n",
    "\n",
    "    if plot_fraud:\n",
    "      # Fraudulent transactions\n",
    "      data_fraud = df.loc[df['isFraud'] == 1]\n",
    "      data_fraud_col = data_fraud[col]\n",
    "\n",
    "      # Non-fraudulent transactions\n",
    "      data_nofraud = df.loc[df['isFraud'] == 0]\n",
    "      data_nofraud_col = data_nofraud[col]\n",
    "\n",
    "      # Create two subplots\n",
    "      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "      # Plot fraudulent histogram in the first subplot\n",
    "      ax1.hist(data_fraud_col, bins=100, color='red', edgecolor='black')\n",
    "      ax1.set_xlabel('Value ($)')\n",
    "      ax1.set_ylabel('Freq.')\n",
    "      ax1.set_title(f'Fraudulent Transactions')\n",
    "\n",
    "      # Plot non-fraudulent histogram in the second subplot\n",
    "      ax2.hist(data_nofraud_col, bins=100, color='blue', edgecolor='black')\n",
    "      ax2.set_xlabel('Value ($)')\n",
    "      ax2.set_ylabel('Freq.')\n",
    "      ax2.set_title(f'Non-Fraudulent Transactions')\n",
    "\n",
    "      # Show the plots\n",
    "      plt.tight_layout()  # Adjust layout for better spacing\n",
    "      plt.show()\n",
    "    else:\n",
    "      data = df[col]\n",
    "      plt.hist(data, bins=100, edgecolor='black')\n",
    "      plt.xlabel('Value ($)')\n",
    "      plt.ylabel('Freq.')\n",
    "      plt.title(f'Histogram of {col} for all Transactions')\n",
    "      plt.show()\n",
    "    \n",
    "  def plot_fraud_per_period(self, df: pd.DataFrame, period_cols: list):\n",
    "    '''\n",
    "    Plot fraud transactions per unit of time (day, month, hour, etc.)\n",
    "    \n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): training dataset to be visualized\n",
    "    -period_cols (list): columns to use for plotting\n",
    "\n",
    "    Displays: Plots as needed\n",
    "    '''\n",
    "\n",
    "    # Create a figure with subplots for each column\n",
    "    fig, axes = plt.subplots(len(period_cols), 1, figsize=(6, 4 * len(period_cols)))\n",
    "    for idx, col in enumerate(period_cols):\n",
    "        # Get fraud data\n",
    "        fraud_df = df.loc[df['isFraud'] == 1]\n",
    "\n",
    "        # Group by the specified column and count occurrences\n",
    "        fraud_grouped = fraud_df.groupby(col)['isFraud'].size()\n",
    "\n",
    "        # Calculate total transactions per category\n",
    "        total_grouped = df.groupby(col)['isFraud'].size()\n",
    "\n",
    "        # Calculate percentages for fraud\n",
    "        fraud_percentage = (fraud_grouped / total_grouped * 100).fillna(0)\n",
    "\n",
    "        # Get the index values (categories) from fraud_percentage\n",
    "        categories = fraud_percentage.index\n",
    "\n",
    "        # Plot the fraud percentages in red on the current subplot (axes[idx])\n",
    "        axes[idx].bar(categories, fraud_percentage, color='red', edgecolor='black')\n",
    "\n",
    "        # Add labels and title\n",
    "        axes[idx].set_xlabel(col)  # Label for the x-axis\n",
    "        axes[idx].set_ylabel('Fraud Percentage (%)')\n",
    "        axes[idx].set_title(f'Fraud Percentage by {col}')\n",
    "    plt.show()\n",
    "  \n",
    "  def plot_correlation(self, df: pd.DataFrame, cols: list):\n",
    "    '''\n",
    "    Plot correlation matrices as needed\n",
    "    \n",
    "    Parameters:\n",
    "    -df (pd.Dataframe): training dataset to be visualized\n",
    "    -cols (list): columns to use for plotting correlation matrix\n",
    "\n",
    "    Displays: Plots as needed\n",
    "    '''\n",
    "\n",
    "    subset = df[cols]\n",
    "    corr_matrix = subset.corr()\n",
    "    plt.figure(figsize=(13, 13))  \n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1, square=True)\n",
    "    plt.title('Correlation Heatmap of Selected Columns')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceDeminesion():  # Due to size of dataset, apply dimensionality reduction steps\n",
    "  def __init__(self, traindata: pd.DataFrame, testdata: pd.DataFrame):\n",
    "    '''\n",
    "    Initialize class\n",
    "    \n",
    "    Parameters:\n",
    "    -traindata (pd.Dataframe): training dataset\n",
    "    -testdata (pd.Dataframe): testing dataset\n",
    "    '''\n",
    "\n",
    "    self.traindata = traindata\n",
    "    self.testdata = testdata\n",
    "    self.v_cols = []\n",
    "    \n",
    "  def plot_and_reduceD(self, plots=True):\n",
    "    '''\n",
    "    Plot scree plot and apply Principal-Component-Analysis to ensure important correlations are extracted\n",
    "    \n",
    "    Parameters:\n",
    "    -plots (bool): Whether of not to show plots (scree plot)\n",
    "\n",
    "    Displays: Plots as needed\n",
    "\n",
    "    Retruns:\n",
    "    -data_final (pd.Dataframe): training dataset with reduced dimensions\n",
    "    -test_data_final (pd.Dataframe): testing dataset with reduced dimensions\n",
    "    '''\n",
    "\n",
    "    v_cols = [col for col in self.traindata.columns if re.match(r'^V\\d+$', col)]\n",
    "    \n",
    "    # PCA on training data\n",
    "    v_data = self.traindata[v_cols]\n",
    "    pca = PCA().fit(v_data)  # Fit PCA on training data\n",
    "    v_data_pca = pca.transform(v_data)  # Transform training data\n",
    "    explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "    # Apply PCA to test data using the same PCA model\n",
    "    v_test_data = self.testdata[v_cols]\n",
    "    v_test_data_pca = pca.transform(v_test_data)  # Transform test data using fitted PCA model\n",
    "\n",
    "    # Number of components to retain based on Kaiser criterion\n",
    "    num_components_kaiser = sum(eigenvalue > 1 for eigenvalue in pca.explained_variance_)\n",
    "    print(f\"Number of components to retain (Kaiser criterion): {num_components_kaiser}\")\n",
    "\n",
    "    if plots:\n",
    "      # Scree plot\n",
    "      plt.figure(figsize=(10, 6))\n",
    "      plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "      plt.xlabel('Principal Component')\n",
    "      plt.ylabel('Explained Variance Ratio')\n",
    "      plt.title('Scree Plot')\n",
    "      plt.grid()\n",
    "\n",
    "      # Zoomed-in scree plot (x-axis up to 30)\n",
    "      plt.figure(figsize=(10, 6))\n",
    "      plt.plot(range(1, 31), pca.explained_variance_ratio_[:30], marker='o', linestyle='--')\n",
    "      plt.xlabel('Principal Component')\n",
    "      plt.ylabel('Explained Variance Ratio')\n",
    "      plt.title('Zoomed-in Scree Plot (First 30 Components)')\n",
    "      plt.grid()\n",
    "\n",
    "      # Cumulative explained variance plot\n",
    "      plt.figure(figsize=(10, 6))\n",
    "      plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "      plt.xlabel('Number of Principal Components')\n",
    "      plt.ylabel('Cumulative Explained Variance')\n",
    "      plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "      plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "      plt.text(12, 0.87, '90% mark', color='red', fontsize=10)\n",
    "      plt.grid()\n",
    "      plt.show()\n",
    "\n",
    "    # Determine the number of components to retain for 90% variance\n",
    "    comp_90 = next(i for i, total in enumerate(explained_variance) if total >= 0.90) + 1\n",
    "    print(f\"\\nNumber of components to retain for 90% variance: {comp_90}\")\n",
    "    print(f\"Cumulative Explained Variance for the {comp_90}th component: {explained_variance[comp_90-1]:.4f}\\n\")\n",
    "\n",
    "    # Transform training data into the reduced-dimensional PCA space\n",
    "    v_data_pca_df = pd.DataFrame(v_data_pca[:, :comp_90], columns=[f'PC{i+1}' for i in range(comp_90)])\n",
    "    data_final = pd.concat([self.traindata.drop(columns=v_cols).reset_index(drop=True), v_data_pca_df], axis=1)\n",
    "\n",
    "    # Transform test data into the same reduced-dimensional PCA space\n",
    "    v_test_data_pca_df = pd.DataFrame(v_test_data_pca[:, :comp_90], columns=[f'PC{i+1}' for i in range(comp_90)])\n",
    "    test_data_final = pd.concat([self.testdata.drop(columns=v_cols).reset_index(drop=True), v_test_data_pca_df], axis=1)\n",
    "\n",
    "    # Return both the transformed training and test datasets\n",
    "    return data_final, test_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class applyModel():\n",
    "  def __init__(self, eval=True):\n",
    "    \"\"\"\n",
    "    Initialize class\n",
    "\n",
    "    Parameters:\n",
    "    -y_test: if model evaluation should be shown\n",
    "    \"\"\"\n",
    "\n",
    "    self.eval = eval\n",
    "\n",
    "  def evaluateModel(self, y_test, y_pred, y_pred_proba):\n",
    "    \"\"\"\n",
    "    evaluate the model using various metrics and print the results\n",
    "\n",
    "    Parameters:\n",
    "    -y_test: testing labels\n",
    "    -y_pred: predicted labels\n",
    "    -y_pred_proba: predicted probabilities for the positive class\n",
    "    \"\"\"\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    # Basic Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Additional Metrics\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print Metrics\n",
    "    print('\\n\\n -------------Model Evaluation Metrics-------------')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "    print(f'Balanced Accuracy: {balanced_acc:.4f}')\n",
    "    print(f'Matthew\\'s Correlation Coefficient: {mcc:.4f}')\n",
    "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc, balanced_acc, mcc, kappa]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "  def trainModel(self, X_train_split: pd.DataFrame, X_val_split: pd.DataFrame, y_train_split: pd.DataFrame, y_val_split: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train model on dataset, and use validation datasets as reference for training to apply early stopping during training intervals\n",
    "\n",
    "    Parameters:\n",
    "    -X_train_split (pd.DataFrame): X features of the training dataset\n",
    "    -X_val_split (pd.DataFrame): X features of the validation dataset\n",
    "    -y_train_split (pd.DataFrame): y features of the trainig dataset\n",
    "    -y_val_split (pd.DataFrame): y features of the validation dataset\n",
    "\n",
    "    Displays: XGB model evaluation if eval was marked as true when class was called\n",
    "\n",
    "    Returns:\n",
    "    -xgb_model (xgb.XGBClassifier): XGB trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # Weigh classes differently since majority are \"safe\" transactions leading to class imbalance\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "    # Define the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',        # Binary classification\n",
    "        eval_metric='auc',                  # Log loss metric for binary classification\n",
    "        use_label_encoder=False,            # Disable the label encoder warning\n",
    "        learning_rate=0.05,                  # Learning rate, same as your NN learning rate\n",
    "        n_estimators=5000,                  # Number of boosting rounds (trees)\n",
    "        max_depth=6,                        # Maximum depth of trees\n",
    "        subsample=0.8,                      # Subsample ratio of the training instances\n",
    "        colsample_bytree=0.8,               # Subsample ratio of columns\n",
    "        scale_pos_weight=scale_pos_weight   # Use scale_pos_weight for imbalanced datasets if needed\n",
    "    )\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    xgb_model.fit(\n",
    "        X_train_split, \n",
    "        y_train_split, \n",
    "        eval_set=[(X_val_split, y_val_split)],  # Validation data to monitor overfitting\n",
    "        early_stopping_rounds=10,               # Stop early if no improvement\n",
    "        verbose=False                           # Print progress (Toggle true/false)\n",
    "    )\n",
    "\n",
    "    # Predict probabilities of fraud for the val/test set\n",
    "    yval_pred = xgb_model.predict(X_val_split)\n",
    "    yval_pred_proba = xgb_model.predict_proba(X_val_split)[:, 1]  # Get probabilities for the positive class (fraud)\n",
    "\n",
    "    if self.eval:\n",
    "      metrics = applyModel.evaluateModel(self, y_val_split, yval_pred, yval_pred_proba)\n",
    "\n",
    "    return xgb_model\n",
    "  \n",
    "  def pred_and_submit(self, model: xgb.XGBClassifier, Xtest: pd.DataFrame, plot_pred=True):\n",
    "    \"\"\"\n",
    "    Apply the model to predict based on the testing dataset and format predictions for submission\n",
    "\n",
    "    Parameters:\n",
    "    -model (xgb.XGBClassifier): XGB trained model\n",
    "    -Xtest (pd.DataFrame): X features of the test dataset\n",
    "    -plot_pred (bool): whether to plot predictions historgram\n",
    "\n",
    "    Displays: predictions histogram if plot_pred=True\n",
    "\n",
    "    Returns:\n",
    "    -submission (pd.DataFrame): predictions from model in submission format as a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict probabilities of fraud for the test set\n",
    "    y_pred_proba = model.predict_proba(Xtest)[:, 1]  # Get probabilities for the positive class (fraud)\n",
    "\n",
    "    # Create the submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "      'TransactionID': test_transaction_df['TransactionID'],\n",
    "      'isFraud': y_pred_proba\n",
    "    })\n",
    "\n",
    "    if plot_pred:\n",
    "      # Plot a histogram of the model predictions if needed\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      plt.hist(submission['isFraud'], bins=100, edgecolor='black')\n",
    "      plt.title('Histogram of Fraud Probabaility')\n",
    "      plt.xlabel('Fraud Probability')\n",
    "      plt.ylabel('Frequency')\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "    # Save the submission DataFrame to CSV if needed\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3.Apply functions on dataset as needed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Read .csv's\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_id_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mieee-fraud-detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain_identity.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m train_transaction_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mieee-fraud-detection\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtrain_transaction.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m test_id_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mieee-fraud-detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_identity.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m test_transaction_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mieee-fraud-detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_transaction.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\parsers.pyx:889\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\parsers.pyx:1034\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\parsers.pyx:1088\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\parsers.pyx:1163\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\dtypes\\common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1332\u001b[0m     )\n\u001b[1;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3.Apply functions on dataset as needed\n",
    "# Read .csv's\n",
    "train_id_df = pd.read_csv(r\"ieee-fraud-detection\\train_identity.csv\")\n",
    "train_transaction_df = pd.read_csv(r\"ieee-fraud-detection\\train_transaction.csv\")\n",
    "test_id_df = pd.read_csv(r\"ieee-fraud-detection\\test_identity.csv\")\n",
    "test_transaction_df = pd.read_csv(r\"ieee-fraud-detection\\test_transaction.csv\")\n",
    "\n",
    "# Store dfs as a list\n",
    "dfs_list = [train_id_df, train_transaction_df, test_id_df, test_transaction_df]\n",
    "\n",
    "# Call preprocessing class\n",
    "PPD = preprocessDatasets()\n",
    "\n",
    "# Iterate through list and remove cols with >90% missing values in the df\n",
    "for df in dfs_list:\n",
    "  PPD.remove_empty_cols(df)\n",
    "\n",
    "# Join ID df and transaction df based on transaction ID\n",
    "train_df = PPD.join_ID(train_transaction_df, train_id_df)\n",
    "test_df = PPD.join_ID(test_transaction_df, test_id_df)\n",
    "\n",
    "# Remove outliers (>3sigma) in transaction amount for model to learn from majority data\n",
    "train_df = PPD.remove_outliers(train_df, 'TransactionAmt')\n",
    "\n",
    "# Feature engineer train & test sets to maintain consistency\n",
    "train_df = PPD.feature_engineer(train_df)\n",
    "test_df = PPD.feature_engineer(test_df)\n",
    "\n",
    "# Replace blanks\n",
    "PPD.replace_blanks(train_df)\n",
    "PPD.replace_blanks(test_df)\n",
    "\n",
    "# Plot common graphs to understand dataset\n",
    "VD = visualizeDataset()\n",
    "VD.plot_hist(train_df, 'TransactionAmt', plot_fraud=True)\n",
    "VD.plot_fraud_per_period(train_df, ['DayOfWeek', 'HourOfDay'])\n",
    "\n",
    "# Collect 'C' & 'D' columns for correlation matrix, adjust cols as needed\n",
    "C_list = []\n",
    "D_list = [f'D{i_0}' for i_0 in range(1, 7)] + [f'D{i_0}' for i_0 in range(8, 16)]\n",
    "\n",
    "for col in train_df.columns:\n",
    "  if col.startswith('C'):\n",
    "    C_list.append(col)\n",
    "\n",
    "# View  different column subsets in correlation matrix as needed\n",
    "VD.plot_correlation(train_df, C_list)\n",
    "VD.plot_correlation(train_df, D_list)\n",
    "\n",
    "# Reduce memory occupied by datasets\n",
    "PPD.reduce_memory(train_df)\n",
    "PPD.reduce_memory(test_df)\n",
    "\n",
    "# Encode and scale datasets appropriately\n",
    "train_df = PPD.encode_df(train_df)\n",
    "test_df = PPD.encode_df(test_df)\n",
    "\n",
    "# Call class for dimensionality reduction since V-column count is very high\n",
    "RD = ReduceDeminesion(train_df, test_df)\n",
    "train_df, test_df = RD.plot_and_reduceD()\n",
    "train_df, test_df = PPD.final_preprocessing(train_df, test_df)  # Apply final preprocessing (drop irrelevant cols, etc.)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train = train_df.drop(columns=['isFraud'])  # Drop the 'isFraud' column to get features\n",
    "y_train = train_df['isFraud']  # Target column\n",
    "X_test = test_df  # The test set does not have 'isFraud'\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "AM = applyModel()\n",
    "xgbModel = AM.trainModel(X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "final_predictions = AM.pred_and_submit(xgbModel, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
